{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de la depresión y la ansiedad en el aborto en Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding libraries\n",
    "\n",
    "# Data operations\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "\n",
    "# Text cleaning\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Date/time handling\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Sentiment Analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "#import nltk\n",
    "#nltk.download('vader_lexicon') # To perform sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Topic extraction\n",
    "from gensim import corpora, models\n",
    "from gensim.matutils import jaccard\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "#import pyLDAvis\n",
    "#from pyLDAvis import gensim\n",
    "#pyLDAvis.enable_notebook()\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracción de datos\n",
    "De la aplicación creada de Twitter Developer obtenemos las siguientes claves de acceso, que nos permitirán acceder a la API de Twitter para obtener los tweets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los dos ficheros obtenidos con el proceso de streaming son:\n",
    "* baby_awareness_day_data.json\n",
    "* streaming_miscarriage_data.json\n",
    "\n",
    "Ya que el proceso de streaming estuvo ejecutando durante un largo periodo de tiempo, en el código a continuación leeremos los ficheros ya generados, que se encuentran en la carpeta `json_files`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tranformación de JSON a CSV (Selección de atributos)\n",
    "\n",
    "Los datos que hemos extraído de twitter, que actualmente se encuentran en formato json, tienen un gran número de atributos, así que necesitamos comprender su contenido y seleccionar los que serán útiles en nuestro análisis posterior. El formato `JSON` nos presenta los datos de un tweet de forma semiestructurada, así que para facilitar nuestro análisis posterior, el primer paso será presentar los datos en formato estructurado añadiéndolos en un `dataframe` de `pandas` y guardándolos finalmente en formato `CSV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orignal files paths\n",
    "path_json = 'dataCollection/tweets.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list()\n",
    "with open(path_json, 'r') as f:\n",
    "    for l in f:\n",
    "        tweets.append(json.loads(l))\n",
    "        \n",
    "# Read JSON into Pandas dataframe\n",
    "df = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_dict_col(df, col):\n",
    "    df2 = pd.json_normalize(df[col])\n",
    "    df2.columns = ['{}.{}'.format(col, i) for i in df2.columns]\n",
    "    df = df.join(df2)\n",
    "    del df[col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = flat_dict_col(df, 'author')\n",
    "#df = flat_dict_col(df, 'public_metrics')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Transformación\n",
    "\n",
    "Ahora ya hemos seleccionado las columnas que necesitaremos para el análisis posterior, pero debemos realizar transformaciones a las columnas para facilitar su uso. Las transformaciones que realizaremos son:\n",
    "\n",
    "* Filtrado de tweets que no estén en inglés\n",
    "* Transformación del formato de las columnas de fecha (creación de las columnas \"year\", \"month\", \"day\", \"hour\" y \"day_of_week\" para el análisis temporal)\n",
    "* Creación de una columna que indique si el tweet es una respuesta de otro tweet\n",
    "* Creación de una columna indicando si el tweet contiene contenido multimedia (photo, video o gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mentioned tranformations\n",
    "# Filtering out tweets not in English\n",
    "def filter_English_tweets(df):\n",
    "    df = df[df['lang'] == 'en']\n",
    "    return df\n",
    "\n",
    "# New columns to works easily with dates\n",
    "def dates_transformation(df): \n",
    "    def date_cleaning(raw_date):\n",
    "        return datetime.strftime(\n",
    "            datetime.strptime(raw_date,'%a %b %d %H:%M:%S +0000 %Y'), \n",
    "            '%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "\n",
    "    df['created_at'] = df['created_at'].apply(date_cleaning)\n",
    "    created_at = pd.DatetimeIndex(df['created_at'])\n",
    "\n",
    "    df['year'] = created_at.year\n",
    "    df['month'] = created_at.month\n",
    "    df['day'] = created_at.day\n",
    "    df['day_of_week'] = created_at.weekday\n",
    "    df['hour'] = created_at.hour\n",
    "\n",
    "    df['user_created_at'] = df['user_created_at'].apply(date_cleaning)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Getting if the tweet is a reply or not\n",
    "def is_reply(df):\n",
    "    df['is_reply'] =  df['in_reply_to_status_id'].apply(lambda x: False if np.isnan(x) else True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Extracting if tweet contains photo, video or gif (or none)\n",
    "def media_extraction(df):\n",
    "    clean_entities = df['extended_tweet_entities_media'].fillna(value=\"\").astype(str)\n",
    "    df['extended_tweet_entities_media'] = clean_entities\n",
    "                        \n",
    "    df['has_photos'] = df['extended_tweet_entities_media'].str.contains(\"'type': 'photo'\")\n",
    "    df['has_videos'] = df['extended_tweet_entities_media'].str.contains(\"'type': 'video'\")\n",
    "    df['has_gifs'] = df['extended_tweet_entities_media'].str.contains(\"'type': 'animated_gif'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Removing extra spaces from all texts\n",
    "def remove_extra_spaces(df):\n",
    "    df = df.replace(r'\\n',' ', regex=True) \n",
    "    df = df.replace(r'/\\|/g',' ', regex=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply all transformations\n",
    "def apply_transformations(df):\n",
    "\n",
    "    df = filter_English_tweets(df) # Get only tweets in English\n",
    "    #df = dates_transformation(df) # Create date columns\n",
    "    #df = is_reply(df) # Create is_reply column\n",
    "    #df = media_extraction(df) # Extract if we have media\n",
    "    df = remove_extra_spaces(df) # Removing messy spaces from texts\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all transformation to dataframes\n",
    "df = apply_transformations(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Limpieza del texto\n",
    "\n",
    "A parte de las columnas transformadas anteriormente, tenemos que realizar un proceso de limpieza específico para la columna que contiene el texto del tweet.\n",
    "\n",
    "En primer lugar, ya que los tweets contienen su texto en 2 campos diferentes dependiendo si están truncados o no, debemos crear un campo que use el texto normal o el extendido dependidiendo del tipo de tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting text or extended text (long tweets, more than 40 chars)\n",
    "df['final_text'] = df['text']\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación debemos aplicar las siguientes transformaciones para limpiar el texto de los tweets:\n",
    "* Texto todo en minúscula\n",
    "* Eliminación de usernames (@username)\n",
    "* Eliminación de hashtags (#hashtag)\n",
    "* Eliminación de hipervínculos (https...)\n",
    "* Eliminación de dígitos\n",
    "* Eliminación de signos de puntuación y espacios extra\n",
    "* Eliminación de palabras de 1 carácter\n",
    "* Tokenization\n",
    "* Eliminación de stopwords\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions the mentioned text cleaning steps\n",
    "def lowercase_text(sentence):\n",
    "    return sentence.lower()\n",
    "\n",
    "def remove_usernames(sentence):\n",
    "    sentence = re.sub(r'@\\w+',' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def remove_hashtags(sentence):\n",
    "    sentence = re.sub(r\"#\\w+\",' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def remove_links(sentence):\n",
    "    sentence = re.sub(r'\\bhttps?://[^\\s]*',' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def remove_numbers(sentence):\n",
    "    sentence = re.sub(r'\\d+',' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def remove_punctuation_and_spaces(sentence):\n",
    "    sentence = re.sub(r'&amp;', ' ', sentence) # Specific for &amp;\n",
    "    sentence = re.sub(r'[^\\w\\s]',' ', sentence)\n",
    "    sentence = ' '.join(sentence.split())\n",
    "    return sentence\n",
    "\n",
    "def remove_1letter_words(sentence):\n",
    "    sentence = re.sub(r'\\b\\w{1}\\b', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def tokenize(sentence):\n",
    "    list_of_words = sentence.split(\" \")\n",
    "    return list_of_words\n",
    "\n",
    "def remove_stopwords(list_of_words):\n",
    "    # Getting English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    filtered_words = []\n",
    "    for word in list_of_words: \n",
    "        if word not in stop_words: \n",
    "            filtered_words.append(word) \n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "def lemmatize(list_of_words):\n",
    "    # Getting English lemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemma_words = []\n",
    "    for w in list_of_words:\n",
    "        word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\") # Names\n",
    "        word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\") # Verbs\n",
    "        word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\") # Adjectives\n",
    "        lemma_words.append(word3)\n",
    "\n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply all text tranformations\n",
    "def text_cleaning(df):\n",
    "    '''\n",
    "    Given a pandas dataframe with tweets returns a list of clean tweet texts\n",
    "    @input: pandas dataframe with tweets\n",
    "    @output: list of clean tweets (same order as in df)\n",
    "    '''\n",
    "    lists_of_text = []\n",
    "\n",
    "    for text in df['final_text']:\n",
    "        sentence = str(text)\n",
    "\n",
    "        # Text transformations\n",
    "        sentence = lowercase_text(sentence) # All in lowercase\n",
    "        sentence = remove_links(sentence) # Remove links\n",
    "        sentence = remove_hashtags(sentence) # Remove Hashtags\n",
    "        sentence = remove_usernames(sentence) # Remove usernames\n",
    "        sentence = remove_numbers(sentence) # Remove numbers\n",
    "        sentence = remove_punctuation_and_spaces(sentence) # Remove punctuation signs and multiple spaces\n",
    "        sentence = remove_1letter_words(sentence) # Remove words with len=1\n",
    "        sentence = remove_punctuation_and_spaces(sentence)\n",
    "        \n",
    "        list_of_words = tokenize(sentence) # Tokenization\n",
    "        list_of_words = remove_stopwords(list_of_words) # Remove stopwords\n",
    "        list_of_words = lemmatize(list_of_words) # Lemmatization\n",
    "        \n",
    "        # Converting list of words to text again\n",
    "        lists_of_text.append(' '.join(list_of_words))\n",
    "\n",
    "    return lists_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de textos\n",
    "df['clean_text'] = text_cleaning(df)\n",
    "df['clean_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Filtrado de tweets no relacionados\n",
    "\n",
    "Durante el análisis de los tweets hemos podido observar que algunos de ellos contienen las palabras específicadas en la búsqueda, pero que no tratan sobre el miscarriage como tal. A continuación se aplica un filtrado de tweets para intentar eliminar algunos de los que no tratan sobre el tema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST WORDS\n",
    "#must_words = r'\\b(?:miscarriage|baby|loss|pregnancy)\\b'\n",
    "\n",
    "# FLAGGED WORDS (UNRELATED TWEETS)\n",
    "flagged_words = [\n",
    "    r'\\bmiscarriage\\s+of\\s+justice\\b',\n",
    "    r'\\bjustice\\b',\n",
    "    r'weight\\s*loss',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to filter unrelated tweets\n",
    "def filter_unrelated_tweets_flagged(df):\n",
    "    # Filtering tweets with FLAGGED words\n",
    "    for flagged_word in flagged_words:\n",
    "        df = df[~df['clean_text'].str.contains(flagged_word, regex=True, flags=re.IGNORECASE)]\n",
    "\n",
    "    return df    \n",
    "\n",
    "def filter_unrelated_tweets(df):\n",
    "    '''\n",
    "    Given a pandas dataframe with tweets, it returns the dataframe filtered \n",
    "    making sure the must words appear or the flagged words are not in the tweet\n",
    "    @input: pandas dataframe with tweets\n",
    "    @output: fitlered pandas dataframe with tweets\n",
    "    '''\n",
    "    # Filtering tweets without MUST words\n",
    "    df = df[df['clean_text'].str.contains(must_words, regex=True, flags=re.IGNORECASE)]\n",
    "\n",
    "    # Filtering tweets with FLAGGED words\n",
    "    for flagged_word in flagged_words:\n",
    "        df = df[~df['clean_text'].str.contains(flagged_word, regex=True, flags=re.IGNORECASE)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out unrelated tweets\n",
    "print(\"Before filtering: \", len(df))\n",
    "\n",
    "df = filter_unrelated_tweets_flagged(df)\n",
    "#df_streaming = filter_unrelated_tweets(df_streaming)\n",
    "\n",
    "print(\"After filtering: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelado: Análisis de sentimiento y extracción de temas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Análisis de sentimiento\n",
    "\n",
    "Usaremos la librería NLTK para el cálculo de la polaridad del tweet, y la librería TextBlob para el cálculo de la subjetividad, y guardaremos los resultados en nuevas columnas de los dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK function for Polarity\n",
    "def sentiment_analysis_polarity_NLTK(df):\n",
    "    '''\n",
    "    Given a dataframe with tweets, it add 4 new columns to express sentiment polarity:\n",
    "    negative score, positive score, neutral score and compound\n",
    "    '''\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    tweets = df['clean_text']\n",
    "\n",
    "    negative_values = []\n",
    "    neutral_values = []\n",
    "    positive_values = []\n",
    "    compound_values = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        polarity_dict = sia.polarity_scores(tweet)\n",
    "        \n",
    "        negative_values.append(polarity_dict['neg'])\n",
    "        neutral_values.append(polarity_dict['neu'])\n",
    "        positive_values.append(polarity_dict['pos'])\n",
    "        compound_values.append(polarity_dict['compound'])\n",
    "\n",
    "    df['polarity_negative'] = negative_values\n",
    "    df['polarity_positive'] = positive_values\n",
    "    df['polarity_neutral'] = neutral_values\n",
    "    df['polarity_compound'] = compound_values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob function for Subjectivity\n",
    "def sentiment_analysis_TextBlob(df):\n",
    "    tweets = df['clean_text']\n",
    "\n",
    "    polarity_list = []\n",
    "    subjectivity_list = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        sentiment = TextBlob(tweet).sentiment\n",
    "        polarity = sentiment.polarity\n",
    "        polarity_list.append(polarity)\n",
    "\n",
    "        subjectivity = sentiment.subjectivity\n",
    "        subjectivity_list.append(subjectivity)\n",
    "\n",
    "    df['polarity'] = polarity_list\n",
    "    df['subjectivity'] = subjectivity_list\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting polarity and subjectivity for all tweets\n",
    "df = sentiment_analysis_polarity_NLTK(df)\n",
    "\n",
    "\n",
    "df = sentiment_analysis_TextBlob(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Extracción de temas (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Entrenamiento del modelo baseline\n",
    "\n",
    "A continuación crearemos un diccionario (id2word) y una Bag Of Words para poder entrenar un modelo LDA. El número de temas y los otros hiperparámetros se seleccionaran aleatoriamente en este apartado, ya que en el siguiente realizaremos un hiperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LDA Model for Awareness Day\n",
    "k = 5\n",
    "lda_model_awareness = LdaModel(bow_awareness,\n",
    "                      num_topics = k,\n",
    "                      id2word = id2word_awareness,\n",
    "                      random_state = 1,\n",
    "                      passes=10)\n",
    "\n",
    "print(lda_model_awareness.print_topics())\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_awareness, texts=df_awareness['review_text'], dictionary=id2word_awareness, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)\n",
    "\n",
    "# Compute Perplexity\n",
    "print('Perplexity Score: ', lda_model_awareness.log_perplexity(bow_awareness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Hyperparameter tuning\n",
    "\n",
    "Hemos entrenado dos modelos LDA con un número de temas al azar y sin seleccionar los hiperparámetros. Para evaluar un modelo de LDA usamos la \"Topic Coherence\", que mide el grado de similaridad semántica entre las palabras con mayor peso dentro de un tema. \n",
    "\n",
    "Por lo tanto, los mejores parámetros para el modelo LDA deben maximizar la coherencia.\n",
    "\n",
    "Adicionalmente, también buscaremos minimizar la repetición de palabras entre temas, y para medir exta repetición usaremos la similaridad de Jaccard, que mide la similitud entre dos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating models changing number of topics\n",
    "# Split words\n",
    "df[\"review_text\"] = df[\"clean_text\"].map(lambda x: x.split(' '))\n",
    "\n",
    "# Word dictionary\n",
    "id2word = corpora.Dictionary(df['review_text'])\n",
    "\n",
    "# Corpus\n",
    "corpus = df['review_text']\n",
    "\n",
    "# Bag of Words\n",
    "bow = [id2word.doc2bow(tweet) for tweet in df['review_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_test = models.LdaMulticore(corpus=corpus, \n",
    "                                        id2word=id2word,\n",
    "                                        num_topics=5, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Considering 1-9 topics\n",
    "num_topics = list(range(1, 10))\n",
    "num_keywords = 15\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}\n",
    "for i in num_topics:\n",
    "    \n",
    "    LDA_models[i] = models.LdaMulticore(corpus=corpus, \n",
    "                                        id2word=id2word,\n",
    "                                        alpha='auto',\n",
    "                                        num_topics=i, workers = 4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get topics to calculate Jaccard similarity\n",
    "    returned_topics = LDA_models[i].show_topics(num_topics=i,num_words=num_keywords,formatted=False)\n",
    "    print('done the {} topic'.format(i))\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in returned_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the models:\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "for m in LDA_models:\n",
    "    print(LDA_models[m])\n",
    "    temp_file = datapath(\"lda_model_{}\".format(m))\n",
    "    LDA_models[m].save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating avg jaccard similarity between topics\n",
    "LDA_distance = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_similarities = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]):\n",
    "        similarities = []\n",
    "        #print(topic1)\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]):\n",
    "            similarities.append(jaccard(topic1, topic2))    \n",
    "        \n",
    "        jaccard_similarities.append(similarities)    \n",
    "\n",
    "    LDA_distance[num_topics[i]] = jaccard_similarities\n",
    "                \n",
    "mean_similarities = [np.array(LDA_distance[i]).mean() for i in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating coherence for all LDA models\n",
    "coherences = [CoherenceModel(model=LDA_models[n], texts=corpus, dictionary=dictionary, coherence='c_v').get_coherence()\\\n",
    "              for n in num_topics[:-1]]\n",
    "perplexities = [LDA_models[n].log_perplexity(bow) for n in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting ideal number of topics\n",
    "ideal_topic_num = num_topics[np.argmax(coherences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting metrics across number of topics\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_similarities, label='Jaccard Distance')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "y_max = max(max(mean_similarities), max(coherences)) + (0.10 * max(max(mean_similarities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "                \n",
    "ax.axes.set_title('Métricas por número de temas', fontsize=25)\n",
    "ax.set_ylabel('Métrica')\n",
    "ax.set_xlabel('Numero de temas')\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same process for Streaming dataset\n",
    "\n",
    "# Creating models changing number of topics\n",
    "dictionary = id2word_streaming\n",
    "bow = bow_streaming\n",
    "corpus = df_streaming['review_text']\n",
    "\n",
    "# Considering 1-9 topics\n",
    "num_topics = list(range(1, 10))\n",
    "num_keywords = 15\n",
    "\n",
    "LDA_models = {}\n",
    "LDA_topics = {}for i in num_topics:\n",
    "    LDA_models[i] = LdaModel(corpus=bow,\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=i,\n",
    "                             update_every=1,\n",
    "                             chunksize=len(bow),\n",
    "                             passes=20,\n",
    "                             alpha='auto',\n",
    "                             random_state=42)\n",
    "\n",
    "    # Get topics to calculate Jaccard similarity\n",
    "    returned_topics = LDA_models[i].show_topics(num_topics=i,num_words=num_keywords,formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in returned_topics]\n",
    "\n",
    "\n",
    "# Calculating avg jaccard similarity between topics\n",
    "LDA_distance = {}\n",
    "for i in range(0, len(num_topics)-1):\n",
    "    jaccard_similarities = []\n",
    "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]):\n",
    "        similarities = []\n",
    "        #print(topic1)\n",
    "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]):\n",
    "            similarities.append(jaccard(topic1, topic2))    \n",
    "        \n",
    "        jaccard_similarities.append(similarities)    \n",
    "\n",
    "    LDA_distance[num_topics[i]] = jaccard_similarities\n",
    "                \n",
    "mean_similarities = [np.array(LDA_distance[i]).mean() for i in num_topics[:-1]]\n",
    "\n",
    "# Calculating coherence for all LDA models\n",
    "coherences = [CoherenceModel(model=LDA_models[n], texts=corpus, dictionary=dictionary, coherence='c_v').get_coherence()\\\n",
    "              for n in num_topics[:-1]]\n",
    "perplexities = [LDA_models[n].log_perplexity(bow) for n in num_topics[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting metrics across number of topics\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_similarities, label='Jaccard')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "y_max = max(max(mean_similarities), max(coherences)) + (0.10 * max(max(mean_similarities), max(coherences)))\n",
    "ax.set_ylim([0, y_max])\n",
    "ax.set_xlim([1, num_topics[-1]-1])\n",
    "                \n",
    "ax.axes.set_title('Métricas por número de temas (Streaming)', fontsize=25)\n",
    "ax.set_ylabel('Métrica')\n",
    "ax.set_xlabel('Numero de temas')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Mejor modelo y predicción de temas\n",
    "Por lo tanto, el número de temas ideal para el entrenamiento de nuesto modelo para Awareness es 3 y para Streaming es 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models with best hiperparameters\n",
    "k = 3\n",
    "\n",
    "# LDA Model for Awareness Day\n",
    "lda_model_awareness = LdaModel(bow_awareness,\n",
    "                      num_topics = k,\n",
    "                      id2word = id2word_awareness,\n",
    "                      random_state = 1,\n",
    "                      passes=10)\n",
    "\n",
    "print(\"LDA Awareness\")\n",
    "print(lda_model_awareness.print_topics(-1))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_awareness, texts=df_awareness['review_text'], dictionary=id2word_awareness, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model for Streaming Week\n",
    "k = 4\n",
    "\n",
    "lda_model_streaming = LdaModel(bow_streaming,\n",
    "                      num_topics = k,\n",
    "                      id2word = id2word_streaming,\n",
    "                      random_state = 1,\n",
    "                      passes=10)\n",
    "\n",
    "print(\"\\nLDA Streaming\")\n",
    "print(lda_model_streaming.print_topics(-1))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_streaming, texts=df_streaming['review_text'], dictionary=id2word_streaming, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic number for each tweet (Awareness)\n",
    "topics = []\n",
    "for tweet in df_awareness['review_text']:\n",
    "    bow_tweet = id2word_awareness.doc2bow(tweet)\n",
    "    vector = lda_model_awareness[bow_tweet]\n",
    "    topic = np.argmax(list(zip(*vector))[1])\n",
    "    topics.append(topic)\n",
    "\n",
    "df_awareness['topic'] = topics\n",
    "\n",
    "\n",
    "# Get topic number for each tweet (Streaming)\n",
    "topics = []\n",
    "for tweet in df_streaming['review_text']:\n",
    "    bow_tweet = id2word_streaming.doc2bow(tweet)\n",
    "    vector = lda_model_streaming[bow_tweet]\n",
    "    topic = np.argmax(list(zip(*vector))[1])\n",
    "    topics.append(topic)\n",
    "\n",
    "df_streaming['topic'] = topics\n",
    "print(df_streaming[['clean_text', 'topic']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4. Visualización de los temas extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for Awareness Day dataset\n",
    "vis = gensim.prepare(lda_model_awareness, bow_awareness, id2word_awareness)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for Streaming dataset\n",
    "vis = gensim.prepare(lda_model_streaming, bow_streaming, id2word_streaming)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of observed topics\n",
    "\n",
    "d_topics_awareness = {0: \"awareness\",     # 2 in vis\n",
    "                      1: \"support\",       # 3 in vis\n",
    "                      2: \"Oklahoma case\"}   # 1 in vis\n",
    "                                         \n",
    "d_topics_streaming = {0: \"love/family\",   # 2 in vis\n",
    "                      1: \"getting help\",  # 4 in vis\n",
    "                      2: \"experiencies\",   # 1 in vis\n",
    "                      3: \"vaccine/death\"} # 3 in vis\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temas obtenidos:**\n",
    "\n",
    "[(0, '0.042*\"loss\" + 0.038*\"baby\" + 0.023*\"pregnancy\" + 0.020*\"day\" + 0.017*\"infant\" + 0.016*\"awareness\" + 0.015*\"light\" + 0.014*\"lose\" + 0.012*\"week\" + 0.011*\"today\"'), \n",
    "(1, '0.045*\"baby\" + 0.035*\"singh\" + 0.025*\"loss\" + 0.024*\"support\" + 0.024*\"need\" + 0.023*\"suffer\" + 0.022*\"life\" + 0.022*\"hear\" + 0.021*\"raise\" + 0.020*\"daughter\"'), \n",
    "(2, '0.069*\"miscarriage\" + 0.024*\"woman\" + 0.010*\"say\" + 0.009*\"go\" + 0.008*\"know\" + 0.008*\"people\" + 0.008*\"like\" + 0.008*\"get\" + 0.008*\"year\" + 0.007*\"fuck\"')]\n",
    "\n",
    "[(0, '0.070*\"baby\" + 0.070*\"loss\" + 0.029*\"sorry\" + 0.011*\"love\" + 0.010*\"lose\" + 0.007*\"go\" + 0.007*\"family\" + 0.006*\"day\" + 0.006*\"know\" + 0.006*\"take\"'), \n",
    "(1, '0.029*\"miscarriage\" + 0.028*\"baby\" + 0.019*\"loss\" + 0.018*\"pregnancy\" + 0.017*\"woman\" + 0.015*\"help\" + 0.013*\"get\" + 0.012*\"please\" + 0.012*\"dedicate\" + 0.011*\"mean\"'), \n",
    "(2, '0.070*\"miscarriage\" + 0.012*\"get\" + 0.010*\"say\" + 0.010*\"know\" + 0.010*\"abortion\" + 0.009*\"go\" + 0.009*\"woman\" + 0.009*\"pregnancy\" + 0.008*\"people\" + 0.008*\"like\"'), \n",
    "(3, '0.062*\"miscarriage\" + 0.022*\"woman\" + 0.011*\"vaccine\" + 0.011*\"death\" + 0.009*\"result\" + 0.009*\"year\" + 0.009*\"week\" + 0.008*\"first\" + 0.008*\"covid\" + 0.008*\"study\"')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Aparición de síntomas de depresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing regex expression with depression words\n",
    "depression_words = [\"overwhelmed\", \"exhausted\", \"distressed\", \"anxiety\", \"anxious\", \"tired\",\n",
    "                    \"low\", \"depression\", \"depressed\", \"discouraged\", \"desperate\", \"demotivated\",\n",
    "                    \"insomnia\", \"cry\", \"nervous\", \"worried\", \"lonely\", \"sad\", \"empty\"]\n",
    "\n",
    "depression_words = lemmatize(depression_words)\n",
    "regex_depr = \"|\".join(depression_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering tweets with depression words\n",
    "df_awareness['depression'] = df_awareness['clean_text'].str.contains(regex_depr, regex=True, flags=re.IGNORECASE)\n",
    "df_awareness_depr = df_awareness[df_awareness['clean_text'].str.contains(regex_depr, regex=True, flags=re.IGNORECASE)]\n",
    "df_awareness_nodepr = df_awareness[~df_awareness['clean_text'].str.contains(regex_depr, regex=True, flags=re.IGNORECASE)]\n",
    "\n",
    "print(\"Awareness\")\n",
    "print(\"Avg. Depression polarity \", np.mean(df_awareness_depr['polarity_compound']))\n",
    "print(\"Avg. Non depression polarity \", np.mean(df_awareness_nodepr['polarity_compound']))\n",
    "\n",
    "df_streaming['depression'] = df_streaming['clean_text'].str.contains(regex_depr, regex=True, flags=re.IGNORECASE)\n",
    "df_streaming_depr = df_streaming[df_streaming['clean_text'].str.contains(regex_depr, regex=True, flags=re.IGNORECASE)]\n",
    "df_streaming_nodepr = df_streaming[~df_streaming['clean_text'].str.contains(regex_depr, regex=True, flags=re.IGNORECASE)]\n",
    "\n",
    "print(\"\\nStreaming\")\n",
    "print(\"Avg. Depression polarity \", np.mean(df_streaming_depr['polarity_compound']))\n",
    "print(\"Avg. Non depression polarity \", np.mean(df_streaming_nodepr['polarity_compound']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Correlación entre variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap with all numeric fields\n",
    "merged_df = pd.concat([df_awareness, df_streaming])\n",
    "columns_matrix = ['truncated', 'user_verified', 'user_followers_count', 'user_friends_count', 'user_favourites_count',\n",
    "'user_statuses_count', 'user_geo_enabled', 'day_of_week', 'hour', 'is_reply', 'has_photos', 'has_videos', 'has_gifs',\n",
    "'polarity_negative', 'polarity_positive', 'polarity_neutral', 'polarity_compound', 'subjectivity', 'depression', 'topic']\n",
    "\n",
    "corr = df_streaming[columns_matrix].corr().round(2)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Defining color map\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Plotting correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, annot=True)\n",
    "plt.title(\"Correlación entre variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between polarity and subjectivity\n",
    "df = df_streaming[['polarity_positive', 'polarity_negative', 'polarity_compound', 'subjectivity']]\n",
    "g = sns.pairplot(df)\n",
    "g.fig.suptitle(\"Correlación y distribución entre polaridad y subjetividad\", y=1.08)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between depression and polarity/subjectivity\n",
    "\n",
    "df = df_streaming[['depression', 'polarity_compound', 'subjectivity']]\n",
    "\n",
    "sns.barplot(x=df['depression'], y=df['polarity_compound'])\n",
    "plt.title(\"Polaridad media por depresión\")\n",
    "plt.xlabel(\"Depresión\")\n",
    "plt.ylabel(\"Polaridad media\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "sns.barplot(x=df['depression'], y=df['subjectivity'])\n",
    "plt.title(\"Subjetividad media por depresión\")\n",
    "plt.xlabel(\"Depresión\")\n",
    "plt.ylabel(\"Subjetividad media\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between topic and polarity\n",
    "df = df_streaming[['depression', 'topic', 'polarity_compound']]\n",
    "df['topic'].replace(d_topics_streaming, inplace=True)\n",
    "\n",
    "sns.barplot(x=df['topic'], y=df['polarity_compound'])\n",
    "plt.title(\"Polaridad media por tema (Streaming)\")\n",
    "plt.xlabel(\"Tema\")\n",
    "plt.ylabel(\"Polaridad media\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_depr = df.groupby(['topic', 'depression']).count().reset_index()\n",
    "sns.barplot(x=df_topic_depr['topic'], y=df_topic_depr['polarity_compound'], hue=df_topic_depr['depression'])\n",
    "plt.title(\"Recuento de tweets por tema y depresión (Streaming)\")\n",
    "plt.xlabel(\"Tema\")\n",
    "plt.ylabel(\"Recuento de tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between topic and subjectivity\n",
    "df = df_streaming[['depression', 'topic', 'subjectivity']]\n",
    "df['topic'].replace(d_topics_streaming, inplace=True)\n",
    "\n",
    "sns.barplot(x=df['topic'], y=df['subjectivity'])\n",
    "plt.title(\"Subjetividad media por tema (Streaming)\")\n",
    "plt.xlabel(\"Tema\")\n",
    "plt.ylabel(\"Subjetividad media\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between topic and polarity\n",
    "df = df_awareness[['depression', 'topic', 'polarity_compound']]\n",
    "df['topic'].replace(d_topics_awareness, inplace=True)\n",
    "#df['depression'] = df['depression'].astype(int)\n",
    "\n",
    "sns.barplot(x=df['topic'], y=df['polarity_compound'])\n",
    "plt.title(\"Polaridad media por tema (Awareness)\")\n",
    "plt.xlabel(\"Tema\")\n",
    "plt.ylabel(\"Polaridad media\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_depr = df.groupby(['topic', 'depression']).count().reset_index()\n",
    "sns.barplot(x=df_topic_depr['topic'], y=df_topic_depr['polarity_compound'], hue=df_topic_depr['depression'])\n",
    "plt.title(\"Recuento de tweets por tema y depresión (Awareness)\")\n",
    "plt.xlabel(\"Tema\")\n",
    "plt.ylabel(\"Recuento de tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between topic and subjectivity\n",
    "df = df_awareness[['depression', 'topic', 'subjectivity']]\n",
    "df['topic'].replace(d_topics_awareness, inplace=True)\n",
    "\n",
    "sns.barplot(x=df['topic'], y=df['subjectivity'])\n",
    "plt.title(\"Subjetividad media por tema (Awareness)\")\n",
    "plt.xlabel(\"Tema\")\n",
    "plt.ylabel(\"Subjetividad media\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal polatity and subjectivity\n",
    "df_aux_aw = df_awareness[['hour', 'polarity_compound']]\n",
    "df_aux_aw = df_aux_aw.groupby('hour').mean()\n",
    "\n",
    "df_aux_st = df_streaming[['hour', 'polarity_compound']]\n",
    "df_aux_st = df_aux_st.groupby('hour').mean()\n",
    "\n",
    "plt.plot(df_aux_aw)\n",
    "plt.plot(df_aux_st)\n",
    "plt.title(\"Polaridad media por hora\")\n",
    "plt.xlabel(\"Hora del día\")\n",
    "plt.ylabel(\"Polaridad media\")\n",
    "plt.legend([\"Awareness\", \"Streaming\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal polatity and subjectivity\n",
    "df_aux_aw = df_awareness[['hour', 'subjectivity']]\n",
    "df_aux_aw = df_aux_aw.groupby('hour').mean()\n",
    "\n",
    "df_aux_st = df_streaming[['hour', 'subjectivity']]\n",
    "df_aux_st = df_aux_st.groupby('hour').mean()\n",
    "\n",
    "plt.plot(df_aux_aw)\n",
    "plt.plot(df_aux_st)\n",
    "plt.title(\"Subjetividad media por hora\")\n",
    "plt.xlabel(\"Hora del día\")\n",
    "plt.ylabel(\"Subjetividad media\")\n",
    "plt.legend([\"Awareness\", \"Streaming\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds\n",
    "\n",
    "# 0. Common words in general\n",
    "# Awareness\n",
    "words = \" \".join(df_awareness['clean_text'])\n",
    "words = str(words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Palabras más frecuentes (Awareness)\")\n",
    "plt.show()\n",
    "\n",
    "# Streaming\n",
    "words = \" \".join(df_streaming['clean_text'])\n",
    "words = str(words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Palabras más frecuentes (Streaming)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# We will remove these common words from the following clouds\n",
    "common_words = r'\\b(miscarriage|baby loss|infant loss|pregnancy|pregnant|loss)\\b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, los word clouds de los datasets nos muestran las palabras más comunes, que son comunes en todos los tweets, como por ejemplo miscarriage, loss o pregnancy. En los siguientes word clouds eliminaremos estas palabras frecuentes para poder observar las diferencias semánticas de los diferentes grupos que hemos comparado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Depression words vs not depression words\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(20,8))\n",
    "\n",
    "# Awareness comparison\n",
    "words_depr = \" \".join(df_awareness_depr['clean_text'])\n",
    "words_depr = str(words_depr)\n",
    "words_depr = re.sub(common_words, \" \", words_depr)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words_depr)\n",
    "ax[0,0].imshow(wordcloud)\n",
    "ax[0,0].axis(\"off\")\n",
    "ax[0,0].set_title(\"Awareness + depression\")\n",
    "\n",
    "\n",
    "words_nodepr = \" \".join(df_awareness_nodepr['clean_text'])\n",
    "words_nodepr = str(words_nodepr)\n",
    "words_nodepr = re.sub(common_words, \" \", words_nodepr)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words_nodepr)\n",
    "ax[0,1].imshow(wordcloud)\n",
    "ax[0,1].axis(\"off\")\n",
    "ax[0,1].set_title(\"Awareness + not depression\")\n",
    "\n",
    "\n",
    "\n",
    "# Streaming comparison\n",
    "words_depr = \" \".join(df_streaming_depr['clean_text'])\n",
    "words_depr = str(words_depr)\n",
    "words_depr = re.sub(common_words, \" \", words_depr)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words_depr)\n",
    "ax[1,0].imshow(wordcloud)\n",
    "ax[1,0].axis(\"off\")\n",
    "ax[1,0].set_title(\"Streaming + depression\")\n",
    "\n",
    "\n",
    "words_nodepr = \" \".join(df_streaming_nodepr['clean_text'])\n",
    "words_nodepr = str(words_nodepr)\n",
    "words_nodepr = re.sub(common_words, \" \", words_nodepr)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words_nodepr)\n",
    "ax[1,1].imshow(wordcloud)\n",
    "ax[1,1].axis(\"off\")\n",
    "ax[1,1].set_title(\"Streaming + not depression\")\n",
    "\n",
    "fig.suptitle(\"Wordclouds comparing datasets and depression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Awareness topics\n",
    "df_topic1 = df_awareness[df_awareness['topic'] == 0]\n",
    "df_topic2 = df_awareness[df_awareness['topic'] == 1]\n",
    "df_topic3 = df_awareness[df_awareness['topic'] == 2]\n",
    "\n",
    "fig, ax = plt.subplots(3,1, figsize=(24,14))\n",
    "\n",
    "# Topic 1\n",
    "words = \" \".join(df_topic1['clean_text'])\n",
    "words = str(words)\n",
    "words = re.sub(common_words, \" \", words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "ax[0].imshow(wordcloud)\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(\"Tema 1: awareness\")\n",
    "\n",
    "\n",
    "# Topic 2\n",
    "words = \" \".join(df_topic2['clean_text'])\n",
    "words = str(words)\n",
    "words = re.sub(common_words, \" \", words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "ax[1].imshow(wordcloud)\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(\"Tema 2: support\")\n",
    "\n",
    "\n",
    "# Topic 3\n",
    "words = \" \".join(df_topic3['clean_text'])\n",
    "words = str(words)\n",
    "words = re.sub(common_words, \" \", words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "ax[2].imshow(wordcloud)\n",
    "ax[2].axis(\"off\")\n",
    "ax[2].set_title(\"Tema 3: Oklahoma case\")\n",
    "\n",
    "fig.suptitle(\"Palabras más frecuentes por tema (Awareness)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Streaming topics\n",
    "df_topic1 = df_streaming[df_streaming['topic'] == 0]\n",
    "df_topic2 = df_streaming[df_streaming['topic'] == 1]\n",
    "df_topic3 = df_streaming[df_streaming['topic'] == 2]\n",
    "df_topic4 = df_streaming[df_streaming['topic'] == 3]\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(20,8))\n",
    "\n",
    "# Topic 1\n",
    "words = \" \".join(df_topic1['clean_text'])\n",
    "words = str(words)\n",
    "words = re.sub(common_words, \" \", words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "ax[0,0].imshow(wordcloud)\n",
    "ax[0,0].axis(\"off\")\n",
    "ax[0,0].set_title(\"Tema 1: love/family\")\n",
    "\n",
    "\n",
    "# Topic 2\n",
    "words = \" \".join(df_topic2['clean_text'])\n",
    "words = str(words)\n",
    "words = re.sub(common_words, \" \", words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "ax[0,1].imshow(wordcloud)\n",
    "ax[0,1].axis(\"off\")\n",
    "ax[0,1].set_title(\"Tema 2: getting help\")\n",
    "\n",
    "\n",
    "# Topic 3\n",
    "words = \" \".join(df_topic3['clean_text'])\n",
    "words = str(words)\n",
    "words = re.sub(common_words, \" \", words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "ax[1,0].imshow(wordcloud)\n",
    "ax[1,0].axis(\"off\")\n",
    "ax[1,0].set_title(\"Tema 3: miscarriage experiencies\")\n",
    "\n",
    "\n",
    "# Topic 4\n",
    "words = \" \".join(df_topic4['clean_text'])\n",
    "words = str(words)\n",
    "words = re.sub(common_words, \" \", words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=30,max_font_size=40, relative_scaling=.5, collocations=False).generate(words)\n",
    "ax[1,1].imshow(wordcloud)\n",
    "ax[1,1].axis(\"off\")\n",
    "ax[1,1].set_title(\"Tema 4: vaccines/death\")\n",
    "\n",
    "\n",
    "fig.suptitle(\"Palabras más frecuentes por tema (Streaming)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Ejemplos de tweets de cada tema\n",
    "\n",
    "A continuación mostraremos una selección de tweets de cada tema que hemos identificado en nuestros datos para caracterizar mejor los temas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awareness tweets\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Topic 1: Awareness\n",
    "print(\"Topic 1: Awareness tweets ----------------------\")\n",
    "print((df_awareness[df_awareness['id'] == 1449156130094059520].final_text).to_string())\n",
    "print((df_awareness[df_awareness['id'] == 1449151365582921728].final_text).to_string())\n",
    "print((df_awareness[df_awareness['id'] == 1449146909386584069].final_text).to_string())\n",
    "\n",
    "# Topic 2: Support\n",
    "print(\"\\nTopic 2: Support tweets ----------------------\")\n",
    "print((df_awareness[df_awareness['id'] == 1449007942196473865].final_text).to_string())\n",
    "print((df_awareness[df_awareness['id'] == 1448261198210818052].final_text).to_string())\n",
    "\n",
    "# Topic 3: Oklahoma case\n",
    "print(\"\\nTopic 3: Oklahoma tweets ----------------------\")\n",
    "print((df_awareness[df_awareness['id'] == 1448791454621577217].final_text).to_string())\n",
    "print((df_awareness[df_awareness['id'] == 1448768844550610944].final_text).to_string())\n",
    "print((df_awareness[df_awareness['id'] == 1448752437431619588].final_text).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straming tweets\n",
    "\n",
    "# Topic 1: Love/Family\n",
    "print(\"Topic 1: Love/Family tweets ----------------------\")\n",
    "print((df_streaming[df_streaming['id'] == 1453504151544123395].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1453548826061676547].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1453771475144617987].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1453840403615080452].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1454573664687378433].final_text).to_string())\n",
    "\n",
    "# Topic 2: Getting Help\n",
    "print(\"\\nTopic 2: Getting help tweets ----------------------\")\n",
    "print((df_streaming[df_streaming['id'] == 1455351241530232833].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1453851642063990791].final_text).to_string())\n",
    "\n",
    "# Topic 3 in next chapter\n",
    "\n",
    "# Topic 4: Death/vaccines\n",
    "print(\"\\nTopic 4: Death/vaccines tweets ----------------------\")\n",
    "print((df_streaming[df_streaming['id'] == 1454274431916777478].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1454838759149785092].final_text).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1. Identificación manual de problemas existentes\n",
    "En el Tema 3 del dataset Streaming, que habla de las experiencias reales de mujeres que han sufrido un miscarriage, hemos identificado un gran número de tweets que ponen en evidencia diversos problemas graves entorno al aborto espontáneo. Hemos encontrado problemas económicos, problemas de estigma sobre el tema y problemas en el sector sanitario, tanto enfatizando en la salud física como mental:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 3: Miscarriage experiences\n",
    "print(\"Topic 3: Miscarriage experiencies tweets\")\n",
    "\n",
    "# Economic problems\n",
    "print(\"\\nEconomic problems ----------------------\")\n",
    "print((df_streaming[df_streaming['id'] == 1454187264188186624].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1454495393643913218].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1454084877507129345].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1454635959023243268].final_text).to_string())\n",
    "\n",
    "# Stigma problems \n",
    "print(\"\\nStigma problems ----------------------\")\n",
    "print((df_streaming[df_streaming['id'] == 1454578368075255809].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455041490921660417].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455609810855022599].final_text).to_string())\n",
    "\n",
    "# Healthcare problems\n",
    "print(\"\\nHealthcare problems ----------------------\")\n",
    "print((df_streaming[df_streaming['id'] == 1455147608587087875].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455580227179098115].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1453553962972442626].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1453739381001318400].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1453774695120113665].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1454689448655040513].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455189476695846925].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455381870913282061].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455522245405782023].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455553691285610508].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455578258947420162].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455652323825438724].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1455008932632485896].final_text).to_string())\n",
    "print((df_streaming[df_streaming['id'] == 1453638597660262405].final_text).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving final results in CSV file\n",
    "output_path_awareness = 'csv_files/results/baby_awareness_day_data.csv'\n",
    "output_path_streaming = 'csv_files/results/streaming_miscarriage_data.csv'\n",
    "\n",
    "df_awareness.to_csv(output_path_awareness, sep='|', header=True, index=False, line_terminator='\\n', encoding='utf-8', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC, float_format='%.15f')\n",
    "df_streaming.to_csv(output_path_streaming, sep='|', header=True, index=False, line_terminator='\\n', encoding='utf-8', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC, float_format='%.15f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet frequency by hour\n",
    "df_awareness_freq_day = df_awareness[['hour', 'id']]\n",
    "df_awareness_freq_day = df_awareness_freq_day.groupby('hour').count()\n",
    "\n",
    "df_streaming_freq_day = df_streaming[['hour', 'id']]\n",
    "df_streaming_freq_day = df_streaming_freq_day.groupby('hour').count()\n",
    "\n",
    "plt.plot(df_awareness_freq_day)\n",
    "plt.plot(df_streaming_freq_day)\n",
    "plt.legend([\"Awareness Day\", \"Streaming\"])\n",
    "plt.xlabel(\"Hora del día\")\n",
    "plt.ylabel(\"Recuento de tweets\")\n",
    "plt.title(\"Frecuencia de publicación de tweets por hora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet frequency by day of week\n",
    "df_awareness_freq_day = df_awareness[['day_of_week', 'id']]\n",
    "df_awareness_freq_day = df_awareness_freq_day.groupby('day_of_week').count()\n",
    "\n",
    "df_streaming_freq_day = df_streaming[['day_of_week', 'id']]\n",
    "df_streaming_freq_day = df_streaming_freq_day.groupby('day_of_week').count()\n",
    "\n",
    "plt.plot(df_awareness_freq_day)\n",
    "plt.plot(df_streaming_freq_day)\n",
    "plt.legend([\"Awareness Day\", \"Streaming\"])\n",
    "plt.xlabel(\"Día de la semana\")\n",
    "plt.ylabel(\"Recuento de tweets\")\n",
    "plt.title(\"Frecuencia de publicación de tweets por día de la semana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media in tweets\n",
    "df_awareness_media = df_awareness[['has_photos', 'has_videos', 'has_gifs']]\n",
    "df_awareness_media = df_awareness_media.sum()\n",
    "total_awareness = len(df_awareness)\n",
    "no_media_awareness = total_awareness - df_awareness_media.sum()\n",
    "df_awareness_media['no_media'] = no_media_awareness\n",
    "\n",
    "labels = ['photos', 'videos', 'gifs', 'no media']\n",
    "sizes = df_awareness_media\n",
    "\n",
    "plt.bar(labels, sizes)\n",
    "\n",
    "for i in range(len(sizes)):\n",
    "    plt.annotate(str(sizes[i]), xy=(labels[i],sizes[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.title(\"Recuento de tweets por su contenido multimedia (Awareness)\")\n",
    "plt.xlabel(\"Tipo de multimedia\")\n",
    "plt.ylabel(\"Recuento de tweets\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streaming_media = df_streaming[['has_photos', 'has_videos', 'has_gifs']]\n",
    "df_streaming_media = df_streaming_media.sum()\n",
    "total_streaming = len(df_streaming)\n",
    "no_media_streaming = total_streaming - df_streaming_media.sum()\n",
    "df_streaming_media['no_media'] = no_media_streaming\n",
    "\n",
    "labels = ['photos', 'videos', 'gifs', 'no media']\n",
    "sizes = df_streaming_media\n",
    "\n",
    "plt.bar(labels, sizes)\n",
    "\n",
    "for i in range(len(sizes)):\n",
    "    plt.annotate(str(sizes[i]), xy=(labels[i],sizes[i]), ha='center', va='bottom')\n",
    "\n",
    "plt.title(\"Recuento de tweets por su contenido multimedia (Streaming)\")\n",
    "plt.xlabel(\"Tipo de multimedia\")\n",
    "plt.ylabel(\"Recuento de tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet location\n",
    "df_awareness_location = df_awareness[['place_id', 'id']]\n",
    "df_awareness_location = df_awareness_location.groupby('place_id').count()\n",
    "print(df_awareness_location.sum())\n",
    "\n",
    "df_streaming_location = df_streaming[['place_id', 'id']]\n",
    "df_streaming_location = df_streaming_location.groupby('place_id').count()\n",
    "print(df_streaming_location.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since place is empty in the majority of tweets, let's check user location\n",
    "# Top 10 user locations\n",
    "df_awareness_location = df_awareness[['user_location', 'id']]\n",
    "df_awareness_location = df_awareness_location.groupby('user_location').count()\n",
    "df_awareness_location = df_awareness_location.sort_values(by='id', ascending=False)\n",
    "print(\"Top 10 localizaciones de usuarios (Awareness)\")\n",
    "print(df_awareness_location.head(10))\n",
    "\n",
    "df_streaming_location = df_streaming[['user_location', 'id']]\n",
    "df_streaming_location = df_streaming_location.groupby('user_location').count()\n",
    "df_streaming_location = df_streaming_location.sort_values(by='id', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 localizaciones de usuarios (Streaming)\")\n",
    "print(df_streaming_location.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. number of words per tweet\n",
    "df_awareness_text = df_awareness[\"clean_text\"].map(lambda x: x.split(' '))\n",
    "count_words = df_awareness_text.apply(lambda x: len(x))\n",
    "\n",
    "print(\"Awareness: \", np.mean(count_words))\n",
    "\n",
    "df_streaming_text = df_streaming[\"clean_text\"].map(lambda x: x.split(' '))\n",
    "count_words = df_streaming_text.apply(lambda x: len(x))\n",
    "\n",
    "print(\"Streaming: \", np.mean(count_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg. number of words per tweet per hour\n",
    "df_awareness_text_hour = df_awareness[['hour', 'clean_text']]\n",
    "df_awareness_text_hour['review_text'] = df_awareness_text_hour[\"clean_text\"].map(lambda x: x.split(' '))\n",
    "df_awareness_text_hour['count_words'] = df_awareness_text.apply(lambda x: len(x))\n",
    "\n",
    "df_awareness_text_hour = df_awareness_text_hour.groupby('hour').mean()\n",
    "\n",
    "\n",
    "df_streaming_text_hour = df_streaming[['hour', 'clean_text']]\n",
    "df_streaming_text_hour['review_text'] = df_streaming_text_hour[\"clean_text\"].map(lambda x: x.split(' '))\n",
    "df_streaming_text_hour['count_words'] = df_streaming_text.apply(lambda x: len(x))\n",
    "\n",
    "df_streaming_text_hour = df_streaming_text_hour.groupby('hour').mean()\n",
    "\n",
    "plt.plot(df_awareness_text_hour)\n",
    "plt.plot(df_streaming_text_hour)\n",
    "plt.legend(['Awareness Day', 'Streaming'])\n",
    "plt.title(\"Media de palabras por tweet por hora\")\n",
    "plt.xlabel(\"Hora del día\")\n",
    "plt.ylabel(\"Media de palabras por tweet\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adf7a61c184914cd9449dbe1417360d44336c0ad5ff71c62dab398127e099e63"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
